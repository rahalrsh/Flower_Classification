{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python_version = 3.6.9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from platform import python_version\n",
    "print('python_version = ' + python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version = 1.14.0\n"
     ]
    }
   ],
   "source": [
    "print('tf version = ' + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working dir /Users/rahal/Development/Anaconda/Flowers\n"
     ]
    }
   ],
   "source": [
    "import cv2 # pip install opencv-python\n",
    "import os \n",
    "import random\n",
    "cwd = os.getcwd()\n",
    "print('current working dir ' + cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rahal/Development/Anaconda/Flowers/daisy\n",
      "/Users/rahal/Development/Anaconda/Flowers/dandelion\n",
      "/Users/rahal/Development/Anaconda/Flowers/rose\n",
      "/Users/rahal/Development/Anaconda/Flowers/sunflower\n",
      "/Users/rahal/Development/Anaconda/Flowers/tulip\n",
      "TRAINING_DATA array length = 4323\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']\n",
    "TRAINING_DATA = []\n",
    "IMAGE_SIZE = 128\n",
    "\n",
    "def resize_image(img):\n",
    "    resized_img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    # plt.imshow(resized_img, cmap='gray')\n",
    "    # plt.show()\n",
    "    return resized_img\n",
    "\n",
    "def show_image(img, name):\n",
    "    print('Name = ' + name)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:\n",
    "        path = cwd + '/' + category\n",
    "        print(path)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img_name in os.listdir(path):\n",
    "            try:\n",
    "                img = cv2.imread(path + '/' + img_name, cv2.IMREAD_GRAYSCALE)\n",
    "                resized_img = resize_image(img)\n",
    "                # show_image(resized_img, img_name)\n",
    "                # img is just a 2D or 3D array, depending on gray scale or not\n",
    "                TRAINING_DATA.append([resized_img, class_num])\n",
    "            except Exception as e:\n",
    "                # pass\n",
    "                print('Exception at ' + img_name)\n",
    "                print(e)\n",
    "\n",
    "\n",
    "create_training_data()    \n",
    "print('TRAINING_DATA array length = ' + str(len(TRAINING_DATA)))\n",
    "\n",
    "# Shuffel training data array\n",
    "random.shuffle(TRAINING_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample in TRAINING_DATA:\n",
    "#     print(CATEGORIES[sample[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep to feed TRAINING_DATA to our neural network\n",
    "\n",
    "train_images_X = [] # features\n",
    "train_labels_y = [] # labels\n",
    "\n",
    "for features, label in TRAINING_DATA:\n",
    "    train_images_X.append(features)\n",
    "    train_labels_y.append(label)\n",
    "    \n",
    "# train_images_X have to be a numpy array\n",
    "train_images_X = np.array(train_images_X).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)  # 1 cuz of gray scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "\n",
    "np.save('train_images_X.npy', train_images_X)\n",
    "np.save('train_labels_y.npy', train_labels_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(train_images_X.shape[1:])\n",
    "# print(train_images_X.shape)\n",
    "# print(train_images_X[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4323, 128, 128, 1)\n",
      "(4323, 5)\n",
      "(128, 128, 1)\n",
      "Train on 3890 samples, validate on 433 samples\n",
      "Epoch 1/3\n",
      "3890/3890 [==============================] - 116s 30ms/sample - loss: 0.4864 - acc: 0.7991 - val_loss: 0.4800 - val_acc: 0.8005\n",
      "Epoch 2/3\n",
      "3890/3890 [==============================] - 125s 32ms/sample - loss: 0.4268 - acc: 0.8187 - val_loss: 0.4217 - val_acc: 0.8263\n",
      "Epoch 3/3\n",
      "3890/3890 [==============================] - 173s 44ms/sample - loss: 0.3617 - acc: 0.8470 - val_loss: 0.4225 - val_acc: 0.8245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb53dc6f60>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# load the data\n",
    "X = np.load('train_images_X.npy')\n",
    "y = np.load('train_labels_y.npy')\n",
    "\n",
    "# Normalize data\n",
    "# Easiset way to normalize data is to scale that data\n",
    "# For images we know min=0 and max=255\n",
    "\n",
    "# Keras does not use integer labels for the usual crossentropy loss, instead it expects a binary vector \n",
    "# (called \"one-hot\"), where the vector is just 0's and a 1 over the index of the right class.\n",
    "# You can easily convert your labels to this format with to_categorical\n",
    "\n",
    "train_images_X = X/255.0\n",
    "train_labels_y = to_categorical(y)\n",
    "\n",
    "print(train_images_X.shape)\n",
    "print(train_labels_y.shape)\n",
    "\n",
    "model = Sequential() # simple sequential model\n",
    "\n",
    "# train_images_shape = ''.join(train_images_X.shape[1:])\n",
    "# print('Input Shape: ' + train_images_shape)\n",
    "print(train_images_X.shape[1:])\n",
    "\n",
    "# Generate a 2 x 64 layered converlutional neural network\n",
    "\n",
    "# Layer 1 with input\n",
    "model.add(Conv2D(64, (3,3), input_shape=train_images_X.shape[1:])) # Add the convolutional layer\n",
    "model.add(Activation('relu')) # Rectified Linear Unit\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Conv2D(64, (3,3))) # Add the convolutional layer\n",
    "model.add(Activation('relu')) # Rectified Linear Unit\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# 64 node final dense layer\n",
    "model.add(Flatten()) # Flatten because CNN is 2d and Dense layer is 1D\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "          \n",
    "model.add(Dense(5))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images_X, \n",
    "          train_labels_y, \n",
    "          batch_size=32,\n",
    "          epochs=3,\n",
    "          validation_split=0.1)\n",
    "\n",
    "\n",
    "# Conv2D - https://keras.io/layers/convolutional/\n",
    "# Activations - https://keras.io/activations/\n",
    "# MaxPooling2D - https://keras.io/layers/pooling/#maxpooling2dMaxPooling2D - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "# Keras separates the concerns of saving your model architecture and saving your model weights.\n",
    "\n",
    "# Model weights are saved to HDF5 format. \n",
    "# This is a grid format that is ideal for storing multi-dimensional arrays of numbers\n",
    "\n",
    "# The model structure can be described and saved using two different formats: JSON and YAML.\n",
    "\n",
    "# model.json\n",
    "# model.h5\n",
    "\n",
    "NAME = 'flowers-cnn-64x2-model'\n",
    "MODEL_NAME_JSON = NAME + '.json'\n",
    "WEIGHTS_NAME_H5 = NAME + '.h5'\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(MODEL_NAME_JSON, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(WEIGHTS_NAME_H5)\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "# from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# # load json and create model\n",
    "# json_file = open(MODEL_NAME_JSON, 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "## load weights into new model\n",
    "# loaded_model.load_weights(WEIGHTS_NAME_H5)\n",
    "# print(\"Loaded model from disk\")\n",
    "\n",
    "## Compile loaded model\n",
    "# loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
